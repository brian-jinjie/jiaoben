为什么部署redis集群？
因为redis分布式内存存储服务，同时能够实现服务的高可用和数据的自动备份，同时支持数据永久存储
１．加快网站的访问速度：程序员把网站经常被访问的数据，存储到内存服务器

概况：        
1.配置redis集群(使用redis脚本一,配置多台redis服务器环境)
2.配置redis管理器（redis环境配置脚本二）
3.启动redis集群
4.测试redis集群
5.配置LNMP网站(使用LNMP脚本一)
6.配置php支持连接redis服务器(LNMP脚本二)
7.编写php脚本连接集群存取数据
8.客户端访问网站脚本存取数据
9.配置pex集群
10.配置LB集群
11.配置haproxy调度器集群（keepalived）



redis环境配置脚本一
#redis环境配置脚本(使用时确保脚本与软件包在同一目录下)
#所用软件包redis-4.0.8.tar.gz

#! /bin/bash
ip=$(ifconfig eth0 | awk 'NR==2{print $2}')

yum -y install gcc 
tar -zxf redis-4.0.8.tar.gz
cd redis-4.0.8/
make && make install
cd utils/
./install_server.sh
/etc/init.d/redis_6379 status
netstat -antup | grep :6379
/etc/init.d/redis_6379 stop
sed -i "70s/127.0.0.1/$ip/" /etc/redis/6379.conf
sed -i "815s/#//"     /etc/redis/6379.conf
sed -i "823s/#//"    /etc/redis/6379.conf
sed -i "829s/#//"   /etc/redis/6379.conf
/etc/init.d/redis_6379 start 
#redis-cli -h $ip -p 端口号


redis环境配置脚本二

#redis环境配置脚本(使用时确保脚本与软件包在同一目录下)
#所需软件包:redis-4.0.8.tar.gz redis-3.2.1.gem
#! /bin/bash
yum  -y  install   rubygems
gem install redis-3.2.1.gem
mkdir  /root/bin
tar -zxvf redis-4.0.8.tar.gz
cd  redis-4.0.8/src/
cp  redis-trib.rb   /root/bin/
chmod  +x   /root/bin/redis-trib.rb




Lnmp脚本一
LNMP部署脚本 (使用时确保脚本与软件包在同一目录下)
#所需软件包：　nginx-1.12.2.tar.gz　　gcc openssl-devel pcre-devel　
#! /bin/bash
tar -xf lnmp_soft.tar.gz 
cd lnmp_soft/
yum -y install gcc openssl-devel pcre-devel 　
useradd -s /sbin/nologin  nginx
tar -xvf nginx-1.12.2.tar.gz
cd nginx-1.12.2
./configure \
--user=nginx   --group=nginx \
--with-http_ssl_module   \
--with-http_stub_status_module
make && make install
yum -y install   mariadb   mariadb-server   mariadb-devel  #提供访问msyql服务器功能的软件包，不需要启动mo
yum -y install   php        php-mysql        php-devel
yum -y install   php-fpm
/usr/local/nginx/sbin/nginx
echo "/usr/local/nginx/sbin/nginx" >> /etc/rc.local　
#设置nginx开机自启
chmod +x /etc/rc.local
#systemctl start   mariadb
#systemctl enable  mariadb
systemctl start  php-fpm
systemctl enable php-fpm
#检查软件包及服务是否启动成功
netstat -ntulp | grep 80
rpm -q php php-devel php-mysql
rpm -q php-fpm


LNMP脚本二：使web服务器　php连接redis集群
所需软件包及文件：rediscluster.php　redis-cluster-4.3.0.tgz
　#!/bin/bash
  tar -xf redis-cluster-4.3.0.tgz 
   cd redis-4.3.0/
　　phpize
   ./configure --with-php-config=/usr/bin/php-config
  make && make install
  #ls /usr/lib64/php/modules
  sed -i '728s/;//'  /etc/php.ini
  sed -i '728s/\.\//123/'  /etc/php.ini
  sed -i '730s/;//'  /etc/php.ini
  sed -i '730s/\_dir//'  /etc/php.ini
  sed -i '730s/ext/\/usr\/lib64\/php\/modules\//'
  systemctl restart php-fpm
  php -m | grep -i redis

#################################################
所用机器　
        web服务器　192.168.4.33-44
        redis集群　192.168.4.51－56,58
        redis集群　192.168.4.57
        pxc 192.168.4.71-73
        LB  192.168.4.81-82
        



详细步骤：
步骤一：　

配置redis集群(使用脚本一配置多台redis服务器环境)

步骤二：
配置redis管理
1）部署ruby脚本运行环境
[root@mgm57 ~]#yum  -y  install   rubygems 
[root@mgm57 ~]# which gem
/usr/bin/gem
[root@mgm57 ~]# ls  *.gem
redis-3.2.1.gem
[root@mgm57 ~]#
[root@mgm57 ~]# gem install redis-3.2.1.gem
Successfully installed redis-3.2.1
Parsing documentation for redis-3.2.1
Installing ri documentation for redis-3.2.1
1 gem installed
[root@mgm57 ~]#
2）创建管理集群脚本
[root@mgm57 ~]#mkdir  /root/bin     //创建命令检索目录
[root@mgm57 ~]#tar -zxvf redis-4.0.8.tar.gz
[root@mgm57 ~]#cd  redis-4.0.8/src/
[root@mgm57 ~]#cp  redis-trib.rb   /root/bin/ //创建管理集群脚本
[root@mgm57 ~]#chmod  +x   /root/bin/redis-trib.rb
[root@mgm57 ~]#redis-trib.rb   help  //查看命令帮助

步骤二：
启动redis集群
redis-trib.rb create --replicas 1 192.168.4.51:6379 192.168.4.52:6379 192.168.4.53:6379 192.168.4.54:6379 192.168.4.55:6379 192.168.4.56:6379 192.168.4.58:6379
>>> Creating cluster
>>> Performing hash slots allocation on 7 nodes...
Using 3 masters:
192.168.4.51:6379
192.168.4.52:6379
192.168.4.53:6379
Adding replica 192.168.4.55:6379 to 192.168.4.51:6379
Adding replica 192.168.4.56:6379 to 192.168.4.52:6379
Adding replica 192.168.4.58:6379 to 192.168.4.53:6379
Adding replica 192.168.4.54:6379 to 192.168.4.51:6379
M: 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c 192.168.4.51:6379
   slots:0-5460 (5461 slots) master
M: 5ea5bbcecaa29bc4175638c7083b39f49ec112ef 192.168.4.52:6379
   slots:5461-10922 (5462 slots) master
M: 5017bbea03d53bbdb766aa887df44db1bb33cf7b 192.168.4.53:6379
   slots:10923-16383 (5461 slots) master
S: 217f499152353d119ae5a3c91c7c77d3fb31f2e1 192.168.4.54:6379
   replicates 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c
S: 8b9d7a2c001d43ba978b63adf5bce45b185d0f86 192.168.4.55:6379
   replicates 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c
S: 68844b63d28ad838715003c0a2083475f6780c2c 192.168.4.56:6379
   replicates 5ea5bbcecaa29bc4175638c7083b39f49ec112ef
S: ef5831b771691e6155bf88f50f7a6dfcc6670cf6 192.168.4.58:6379
   replicates 5017bbea03d53bbdb766aa887df44db1bb33cf7b
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join...
>>> Performing Cluster Check (using node 192.168.4.51:6379)
M: 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c 192.168.4.51:6379
   slots:0-5460 (5461 slots) master
   2 additional replica(s)
M: 5ea5bbcecaa29bc4175638c7083b39f49ec112ef 192.168.4.52:6379
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
S: 68844b63d28ad838715003c0a2083475f6780c2c 192.168.4.56:6379
   slots: (0 slots) slave
   replicates 5ea5bbcecaa29bc4175638c7083b39f49ec112ef
S: ef5831b771691e6155bf88f50f7a6dfcc6670cf6 192.168.4.58:6379
   slots: (0 slots) slave
   replicates 5017bbea03d53bbdb766aa887df44db1bb33cf7b
S: 8b9d7a2c001d43ba978b63adf5bce45b185d0f86 192.168.4.55:6379
   slots: (0 slots) slave
   replicates 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c
S: 217f499152353d119ae5a3c91c7c77d3fb31f2e1 192.168.4.54:6379
   slots: (0 slots) slave
   replicates 00de5c7dcb658a3a7f59a7cbc109626f1716ab5c
M: 5017bbea03d53bbdb766aa887df44db1bb33cf7b 192.168.4.53:6379
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.


在任意一台redis服务器本机，查看集群信息
[root@redisA ~]# redis-cli  -h 192.168.4.51 -p 6379
192.168.4.51:6379> cluster info       //查看集群信息
cluster_state:ok
……
……
cluster_known_nodes:6
cluster_size:3
192.168.4.51:6379> cluster  nodes   //查看集群节点信息
d9634ba0aa5c1a07193da4a013da6051c1515922 192.168.4.54:6354@16354 slave 9e44139cffb8ebd7ed746aabbf4bcea9bf207645 0 1561357552212 4 connected
894dd0008053f6fb65e9e4a36b755d9351607500 192.168.4.56:6356@16356 slave 324e05df3f143ef97e50d09be0328a695e655986 0 1561357554216 6 connected
d9f8fe6d6d9dd391be8e7904501db1535e4d17cb 192.168.4.51:6351@16351 myself,master - 0 1561357545000 1 connected 0-5460
324e05df3f143ef97e50d09be0328a695e655986 192.168.4.52:6352@16352 master - 0 1561357553214 2 connected 5461-10922
9e44139cffb8ebd7ed746aabbf4bcea9bf207645 192.168.4.53:6353@16353 master - 0 1561357554216 3 connected 10923-16383
2d343a9df48f6f6e207949e980ef498466a44dad 192.168.4.55:6355@16355 slave d9f8fe6d6d9dd391be8e7904501db1535e4d17cb 0 1561357553716 5 connected
192.168.4.51:6351>

步骤三：

测试集群
[root@client50 ~]# redis-cli  -c  -h 192.168.4.51 -p 6351 //连接服务器51
192.168.4.51:6351>
192.168.4.51:6351> set x 100  //存储
-> Redirected to slot [16287] located at 192.168.4.53:6353  //提示存储在53主机
OK
192.168.4.53:6353> keys *
1) "x"
192.168.4.53:6353>
192.168.4.53:6353> set y 200
OK
192.168.4.53:6353> keys *
1) "y"
2) "x"
192.168.4.53:6353> set z 300 //存储
-> Redirected to slot [8157] located at 192.168.4.52:6352 //提示存储在52主机
OK
192.168.4.52:6352> keys *  //在52主机查看数据 只有变量z 
1) "z"
192.168.4.52:6352> get x 
-> Redirected to slot [16287] located at 192.168.4.53:6353 //连接53主机获取数据
"100"
192.168.4.53:6353> keys *
1) "y"
2) "x"
192.168.4.53:6353> get z
-> Redirected to slot [8157] located at 192.168.4.52:6352
"300"
192.168.4.52:6352> set i 400
-> Redirected to slot [15759] located at 192.168.4.53:6353
OK
192.168.4.53:6353> set j 500
-> Redirected to slot [3564] located at 192.168.4.51:6351
OK
192.168.4.51:6351>

步骤四：
配置LNMP　web网站
 安装部署LNMP软件
＃未修改nginx的php服务
备注：mariadb（数据库客户端软件）、mariadb-server（数据库服务器软件）、mariadb-devel（其他客户端软件的依赖包）
php（解释器）、php-fpm（进程管理器服务）、php-mysql（PHP的数据库扩展包） php-devel。
1）安装软件包
]# yum -y install gcc openssl-devel pcre-devel 
]# useradd -s /sbin/nologin  nginx
]# tar -xvf nginx-1.12.2.tar.gz
]# cd nginx-1.12.2
 nginx-1.12.2]# ./configure   \
--user=nginx   --group=nginx \
--with-http_ssl_module   \
--with-http_stub_status_module
 nginx-1.12.2]# make && make install
]# yum -y install   mariadb   mariadb-server   mariadb-devel
]# yum -y install   php        php-mysql        php-fpm
]# yum -y install   php-devel
2)启动服务(nginx、mariadb、php-fpm)
]# /usr/local/nginx/sbin/nginx                 #启动Nginx服务
]# echo "/usr/local/nginx/sbin/nginx" >> /etc/rc.local
]# chmod +x /etc/rc.local
]# ss -utnlp | grep :80                        #查看端口信息
]# #systemctl start   mariadb                   #启动mariadb服务器
]# #systemctl enable  mariadb                   
]# systemctl start  php-fpm                   #启动php-fpm服务
]# systemctl enable php-fpm

步骤五：
配置ＰＨＰ支持连接redis服务器：
两台web服务器做相同配置

[root@web44 ~]# tar -xf redis-cluster-4.3.0.gz
[root@web44 ~]# cd redis-4.3.0/
[root@web44 redis-4.3.0]# phpize　　#必要步骤，不做会无法做源码安装
Configuring for:
PHP Api Version:         20100412
Zend Module Api No:      20100525
Zend Extension Api No:   220100525
[root@web44 redis-4.3.0]# ./configure --with-php-config=/usr/bin/php-config
[root@web44 redis-4.3.0]#　make && make install
[root@web44 redis-4.3.0]# ls /usr/lib64/php/modules/
curl.so  fileinfo.so  json.so  mysqli.so  mysql.so  pdo_mysql.so  pdo.so  pdo_sqlite.so  phar.so  redis.so  sqlite3.so  zip.so
[root@web44 redis-4.3.0]# vim /etc/php.ini
　　728 extension_dir = "/usr/lib64/php/modules"
  　730　extension = "redis.so"
　　[root@web44 redis-4.3.0]# systemctl restart php-fpm
[root@web44 redis-4.3.0]# php -m | grep -i redis
redis


步骤六：
配置pxc集群三台数据库（71，72，73）
三台主机配置都要配置：
配置服务器192.168.4.71
]#  vim /etc/hosts
192.168.4.71     pxcnode71
192.168.4.72     pxcnode72
192.168.4.73     pxcnode73 
:wq
]#hostname  pxcnode71

配置服务器192.168.4.72
]#  vim /etc/hosts
192.168.4.71     pxcnode71
192.168.4.72     pxcnode72
192.168.4.73     pxcnode73 
:wq
]#hostname  pxcnode72
配置服务器192.168.4.73
]#  vim /etc/hosts
192.168.4.71     pxcnode71
192.168.4.72     pxcnode72
192.168.4.73     pxcnode73 
:wq
]#hostname  pxcnode73

在任意一台服务器上ping 对方的主机名，ping通为配置成功。
[root@host71 ~]# ping -c 2  pxcnode71  //成功
PING pxcnode71 (192.168.4.71) 56(84) bytes of data.
64 bytes from pxcnode71 (192.168.4.71): icmp_seq=1 ttl=255 time=0.011 ms
64 bytes from pxcnode71 (192.168.4.71): icmp_seq=2 ttl=255 time=0.020 ms
--- pxcnode71 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.011/0.015/0.020/0.006 ms
[root@host71 ~]# 
[root@host71 ~]# 
[root@host71 ~]# ping -c 2  pxcnode72 //成功
PING pxcnode72 (192.168.4.72) 56(84) bytes of data.
64 bytes from pxcnode72 (192.168.4.72): icmp_seq=1 ttl=255 time=0.113 ms
64 bytes from pxcnode72 (192.168.4.72): icmp_seq=2 ttl=255 time=0.170 ms
--- pxcnode72 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.113/0.141/0.170/0.030 ms
[root@host71 ~]# 
[root@host71 ~]# 
[root@host71 ~]# ping -c 2  pxcnode73 //成功
PING pxcnode73 (192.168.4.73) 56(84) bytes of data.
64 bytes from pxcnode73 (192.168.4.73): icmp_seq=1 ttl=255 time=0.198 ms
64 bytes from pxcnode73 (192.168.4.73): icmp_seq=2 ttl=255 time=0.155 ms
--- pxcnode73 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.155/0.176/0.198/0.025 ms
[root@host71 ~]#

安装软件包　
1）在192.168.4.71 服务器安装软件包
软件包之间有依赖注意软件包安装顺序
]# rpm -ivh libev-4.15-1.el6.rf.x86_64.rpm    //安装依赖
]# yum  -y  install  percona-xtrabackup-24-2.4.13-1.el7.x86_64.rpm
 ]# rpm -ivh qpress-1.1-14.11.x86_64.rpm     //安装依赖
 ]# tar -xvf  Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar
 ]# yum -y  install  Percona-XtraDB-Cluster-*.rpm
2）在192.168.4.72 服务器安装软件包
]# rpm -ivh libev-4.15-1.el6.rf.x86_64.rpm    //安装依赖
]# yum  -y  install  percona-xtrabackup-24-2.4.13-1.el7.x86_64.rpm
]# rpm -ivh qpress-1.1-14.11.x86_64.rpm     //安装依赖
]# tar -xvf  Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar
]# yum -y  install  Percona-XtraDB-Cluster-*.rpm
3）在192.168.4.73 服务器安装软件包
]# rpm -ivh libev-4.15-1.el6.rf.x86_64.rpm    //安装依赖
]# yum  -y  install  percona-xtrabackup-24-2.4.13-1.el7.x86_64.rpm
]# rpm -ivh qpress-1.1-14.11.x86_64.rpm     //安装依赖
]# tar -xvf  Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar
]# yum -y  install  Percona-XtraDB-Cluster-*.rpm

修改mysqld.cnf文件（只修改server-id即可）
[root@pxcnode71 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld.cnf
[mysqld]
server-id=71                      //server-id 不允许重复
datadir=/var/lib/mysql                  //数据库目录
socket=/var/lib/mysql/mysql.sock         //socket文件
log-error=/var/log/mysqld.log        //日志文件
pid-file=/var/run/mysqld/mysqld.pid    //pid文件
log-bin                    //启用binlog日志
log_slave_updates            //启用链式复制
expire_logs_days=7            //日志文件保留天数
:wq
修改服务器192.168.4.72
[root@pxcnode72 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld.cnf
[mysqld]
server-id=72                      //server-id 不允许重复
datadir=/var/lib/mysql                  //数据库目录
socket=/var/lib/mysql/mysql.sock         //socket文件
log-error=/var/log/mysqld.log        //日志文件
pid-file=/var/run/mysqld/mysqld.pid    //pid文件
log-bin                    //启用binlog日志
log_slave_updates            //启用链式复制
expire_logs_days=7            //日志文件保留天数
:wq
修改服务器192.168.4.73
[root@pxcnode73 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld.cnf
[mysqld]
server-id=73                      //server-id 不允许重复
datadir=/var/lib/mysql                  //数据库目录
socket=/var/lib/mysql/mysql.sock         //socket文件
log-error=/var/log/mysqld.log        //日志文件
pid-file=/var/run/mysqld/mysqld.pid    //pid文件
log-bin                    //启用binlog日志
log_slave_updates            //启用链式复制
expire_logs_days=7            //日志文件保留天数
:wq

分别修改3台服务器的mysqld_safe.cnf （使用默认配置即可）
[root@pxcnode71 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
[mysqld_safe]
pid-file = /var/run/mysqld/mysqld.pid
socket   = /var/lib/mysql/mysql.sock
nice     = 0
:wq
．．．．．．．．．．．．．．．．

分别修改3台服务器的wsrep.cnf文件

1）分别修改3台服务器的wsrep.cnf
[root@pxcnode71 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
wsrep_cluster_address=gcomm://192.168.4.71,192.168.4.72,192.168.4.73//成员列表
wsrep_node_address=192.168.4.71 //本机ip
wsrep_cluster_name=pxc-cluster //集群名
wsrep_node_name=pxcnode71 //本机主机名
wsrep_sst_auth="sstuser:123qqq...A" //SST数据同步授权用户及密码
:wq
修改服务器192.168.4.72
[root@pxcnode72 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
wsrep_cluster_address=gcomm://192.168.4.71,192.168.4.72,192.168.4.73//成员列表
wsrep_node_address=192.168.4.72 //本机ip
wsrep_cluster_name=pxc-cluster //集群名
wsrep_node_name=pxcnode72 //本机主机名
wsrep_sst_auth="sstuser:123qqq...A" //SST数据同步授权用户及密码
:wq
修改服务器192.168.4.73
[root@pxcnode73 ~]# vim /etc/percona-xtradb-cluster.conf.d/mysqld_safe.cnf
wsrep_cluster_address=gcomm://192.168.4.71,192.168.4.72,192.168.4.73//成员列表
wsrep_node_address=192.168.4.73 //本机ip
wsrep_cluster_name=pxc-cluster //集群名
wsrep_node_name=pxcnode73 //本机主机名
wsrep_sst_auth="sstuser:123qqq...A" //SST数据同步授权用户及密码
:wq

启动服务
root@pxcnode71 ~]# ]# systemctl  start mysql@bootstrap.service  //启动集群服务
[root@pxcnode71 ~]# grep pass /var/log/mysqld.log     //查看数据库管理员初始登录密码
2019-06-20T12:29:42.489377Z 1 [Note] A temporary password is generated for root@localhost: W.HiOb8(ok)_
[root@pxcnode71 ~]#mysql –uroot –p’ W.HiOb8(ok)_’ //使用初始密码登录
Mysql> alter user  root@”localhost” identified by “123456”;//修改登录密码
MySQL> exit;
[root@pxcnode71 ~]#mysql –uroot –p123456 //使用修改后的密码登录
Mysql> garnt reload, lock tables,replication client,process on *.*  to
sstuser@"localhost” identified by  “123qqq…A”; //添加授权用户
2）启动数据库服务
启动主机pxcnode72的数据库服务，会自动同步pxcnode71主机的root初始密码和授权用户sstuser
[root@pxcnode72 ~]# systemctl  start mysql  //启动数据库服务
[root@pxcnode72 ~]#
[root@pxcnode72 ~]# netstat -utnlp  | grep :3306
tcp6       0      0 :::3306                 :::*                    LISTEN      12794/mysqld        
[root@pxcnode72 ~]# netstat -utnlp  | grep :4567
tcp        0      0 0.0.0.0:4567            0.0.0.0:*               LISTEN      12794/mysqld        
[root@host72 ~]#
启动主机pxcnode73的数据库服务，会自动同步pxcnode71主机的root初始密码和授权用户sstuser
[root@pxcnode73 ~]# systemctl  start mysql  //启动数据库服务
[root@pxcnode73 ~]#
[root@pxcnode73 ~]# netstat -utnlp  | grep :3306
tcp6       0      0 :::3306                 :::*                    LISTEN      12794/mysqld        
[root@pxcnode73 ~]# netstat -utnlp  | grep :4567
tcp        0      0 0.0.0.0:4567            0.0.0.0:*               LISTEN      12794/mysqld        
[root@host73 ~]#
测试配置
1）启动数据库服务
在任意一台数据查看都可以。
[root@pxcnode71 ~]# mysql -uroot -p123456
wsrep_incoming_addresses 192.168.4.71:3306,192.168.4.72:3306,192.168.4.73:3306 //集群成员列表 
wsrep_cluster_size       3 //集群服务器台数
wsrep_cluster_status   Primary     //集群状态                           
wsrep_connected        ON            //连接状态
wsrep_ready             ON           //服务状态

添加访问数据的连接用户 （在任意一台服务器上添加都可以，另外的2台服务器会自动同步授权用户）
1）添加访问数据的连接用户 （在任意一台服务器上添加都可以，另外的2台服务器会自动同步授权用户）
[root@pxcnode72 ~]# mysql  -uroot  -p123456
mysql> grant all on  gamedb.*  to yaya@"%" identified by "123456"; //添加授权用户
Query OK, 0 rows affected, 1 warning (0.18 sec)
[root@pxcnode71 ~]# mysql -uroot -p123456 -e  'show grants for yaya@"%" ' //查看
mysql: [Warning] Using a password on the command line interface can be insecure.
+--------------------------------------------------+
| Grants for yaya@%                                |
+--------------------------------------------------+
| GRANT USAGE ON *.* TO 'yaya'@'%'                 |
| GRANT ALL PRIVILEGES ON `gamedb`.* TO 'yaya'@'%' |
+--------------------------------------------------+
[root@pxcnode71 ~]#
[root@pxcnode73 ~]# mysql -uroot -p123456 -e  'show grants for yaya@"%" ' //查看
mysql: [Warning] Using a password on the command line interface can be insecure.
+--------------------------------------------------+
| Grants for yaya@%                                |
+--------------------------------------------------+
| GRANT USAGE ON *.* TO 'yaya'@'%'                 |
| GRANT ALL PRIVILEGES ON `gamedb`.* TO 'yaya'@'%' |
+--------------------------------------------------+
[root@pxcnode73 ~]#
2）客户端连接集群存取数据 （连接任意一台数据库服务器的ip地址都可以）
连接数据服务器主机73
client50 ~]# mysql -h192.168.4.73 -uyaya -p123456 //连接服务器73
mysql>
mysql> create database gamedb; //建库
Query OK, 1 row affected (0.19 sec)
mysql>  create table  gamedb.a(id int primary key auto_increment,name char(10));//建表
Query OK, 0 rows affected (1.02 sec)
mysql> insert into gamedb.a(name)values("bob"),("tom"); //插入记录
Query OK, 2 rows affected (0.20 sec)
Records: 2  Duplicates: 0  Warnings: 0
3）在另外2台数据库服务器查看数据，客户端连接数据库服务器71主机查看数据。
client50 ~]# mysql -h192.168.4.71 -uyaya -p123456 //连接服务器71
mysql> select  * from  gamedb.a; //查看记录
+----+-------+
| id | name  |
+----+-------+
|  2 | bob   |
|  5 | tom   |
4）客户端连接数据库服务器73主机查看数据
client50 ~]# mysql -h192.168.4.73 -uyaya -p123456 //连接服务器73
mysql> select  * from  gamedb.a; //查看记录
+----+-------+
| id | name  |
+----+-------+
|  2 | bob   |
|  5 | tom   |

测试故障自动恢复

停止数据库服务
停止3台服务器的任意一台主机的数据库服务都不会影响数据的存取。
[root@pxcnode71 ~]# systemctl  stop  mysql  //停止71主机的数据库服务
Client50 ~]# client50 ~]# mysql -h192.168.4.72 -uyaya -p123456 //连接服务器72
mysql> insert into gamedb.a(name)values("bob2"),("tom2");
mysql> insert into gamedb.a(name)values("jerry"),("jack");
Query OK, 2 rows affected (0.20 sec)
Records: 2  Duplicates: 0  Warnings: 0 
客户端50，连接数据库主机73，查看数据
client50 ~]# mysql -h192.168.4.73 -uyaya -p123456 //连接服务器73
mysql> select  * from  gamedb.a;
+----+-------+
| id | name  |
+----+-------+
|  2 | bob   |
|  5 | tom   |
|  7 | bob2  |
|  9 | tom2  |
| 11 | jerry |
| 13 | jack  |
+----+-------+
6 rows in set (0.00 sec)
3）启动71主机的数据库服务
数据库服务运行后，会自动同步宕机期间的数据。
client50 ~]# mysql -h192.168.4.71 -uyaya -p123456 //连接服务器71
mysql> select  * from  gamedb.a;
+----+-------+
| id | name  |
+----+-------+
|  2 | bob   |
|  5 | tom   |
|  7 | bob2  |
|  9 | tom2  |
| 11 | jerry |
| 13 | jack  |
+----+-------+
rows in set (0.00 sec)



配置数据库服务负载均衡集群(81.82作为调度器)
１．安装haproxy软件
２．修改配置文件
３．启动服务
４．登录管理页面，查看调度信息
５．测试配置
[root@proxy2 ~]# yum -y install haproxy 
[root@proxy2 ~]# vim /etc/haproxy/haproxy.cfg
#．．．．．．．
#(把59行以后的数据删除后，添加下列信息)
listen status
        mode http
        bind *:80
        stats enable
        stats uri /admin
        stats auth admin:admin
        
　　　　　listen mysql_3306 *:3306
        mode tcp
        option tcpka
        balance roundrobin
        server mysql_01 192.168.4.71:3306 check
        server mysql_02 192.168.4.72:3306 check
        server mysql_03 192.168.4.73:3306 check
[root@proxy2 ~]# systemctl start haproxy
[root@proxy2 ~]# systemctl enable haproxy




HA集群（配置调度器的高可用集群，vip地址　192.168.4.250）
准备用调度服务器
1．在两台主机上安装keepalived 
配置第一台代理服务器proxy（192.168.4.81）。
[root@proxy ~]# yum install -y keepalived
[root@proxy ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  router_id  proxy1                        //设置路由ID号
  vrrp_iptables                               //不添加任何防火墙规则
}
vrrp_instance VI_1 {
  state MASTER                         //主服务器为MASTER（备服务器需要修改为BACKUP）
  interface eth0                    //定义网络接口
  virtual_router_id 51                
  priority 100                     //服务器优先级,优先级高优先获取VIP（实验需要修改）
  advert_int 1
  authentication {
    auth_type pass
    auth_pass 1111                       //主备服务器密码必须一致
  }
  virtual_ipaddress {                   //谁是主服务器谁获得该VIP（实验需要修改）
192.168.4.80 
}    
}
[root@proxy ~]# systemctl start keepalived

配置第二台代理服务器proxy（192.168.4.82）。
[root@proxy2 ~]# yum install -y keepalived
[root@proxy2 ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  router_id  proxy2                        //设置路由ID号
vrrp_iptables                               //不添加任何防火墙规则
}
vrrp_instance VI_1 {
  state BACKUP                         //主服务器为MASTER（备服务器需要修改为BACKUP）
  interface eth0                    //定义网络接口
  virtual_router_id 51                
  priority 50                         //服务器优先级,优先级高优先获取VIP
  advert_int 1
  authentication {
    auth_type pass
    auth_pass 1111                       //主备服务器密码必须一致
  }
  virtual_ipaddress {                   //谁是主服务器谁获得该VIP
192.168.4.250
}    
}
[root@proxy2 ~]# systemctl start keepalived

在web33上进行轮询检测

把mysql主从同步结构的数据迁移到pex 集群上
１．停止22从服务器的slave进程　stop slave;
2.使用indbbackupex 对22做数据完全备份
3.把完全备份的文件拷贝到pxc主机做数据恢复
４．启动mysql数据库服务
５．查看slave进程的状态
６．查看数据
[root@web33 html]# mysql -h192.168.4.82 -uyaya -p123456 -e "select @@hostname"
+------------+
| @@hostname |
+------------+
| pxcnode73  |
+------------+
[root@web33 html]# mysql -h192.168.4.82 -uyaya -p123456 -e "select @@hostname"
+------------+
| @@hostname |
+------------+
| pxcnode71  |
+------------+
[root@web33 html]# mysql -h192.168.4.82 -uyaya -p123456 -e "select @@hostname"
+------------+
| @@hostname |
+------------+
| pxcnode72  |
+------------+















